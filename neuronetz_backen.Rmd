---
title: "Estimation of Neural Net"
output: html_notebook
---

### # Installation ggf. noch benötigter Pakete ###
```{r}
# Nur ausführen, beim allerersten Mal !!
#install.packages("fastDummies")
#install.packages("reticulate")

#library(reticulate)
#py_install("tensorflow")
#py_install("pandas")

```


### Vorbereitung der Umgebung ###
```{r}
# Umgebungsvariablen löschen
remove(list = ls())

# Einbinden benötogter Funktionsbibliotheken
library(reticulate)
library(readr)
library(dplyr)
library(stringr)
library(fastDummies)
library(ggplot2)
library(Metrics)


# Funktionsdefinitionen

#' Title Fast creation of normalized variables
#' Quickly create normalized columns from numeric type columns in the inputted data. This function is useful for statistical analysis when you want normalized columns rather than the actual columns.
#'
#' @param .data An object with the data set you want to make normalized columns from.
#' @param norm_values Dataframe of column names, means, and standard deviations that is used to create corresponding normalized variables from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) with same number of rows as inputted data and original columns plus the newly created normalized. columns.
#' @export
#'
#' @examples
norm_cols <- function (.data, norm_values = NULL) {
  for (i in 1:nrow(norm_values)  ) {
    .data$norm <- (.data[[norm_values$name[i]]] - norm_values$mean[i]) / norm_values$sd[i]
    names(.data)[length(.data)] <- paste0(norm_values$name[i], "_norm")
  }
  return (.data)
}


#' Title Creation of a Dataframe including the Information to Standardize Variables
#' This function is meant to be used in combination with the function norm_cols
#'
#' @param .data A data set including the variables you want to get the means and standard deviations from.
#' @param select_columns A vector with a list of variable names for which you want to get the means and standard deviations from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) including the names, means, and standard deviations of the variables included in the select_columns argument.
#' @export
#'
#' @examples
get.norm_values <- function (.data, select_columns = NULL) {
  result <- NULL
  for (col_name in select_columns) {
    mean <- mean(.data[[col_name]])
    sd <- sd(.data[[col_name]])
    result <- rbind (result, c(mean, sd))
  }
  result <- as.data.frame(result, stringsAsFactors = FALSE)
  result <- data.frame (select_columns, result, stringsAsFactors = FALSE)
  names(result) <- c("name", "mean", "sd")
  return (result)
}


py_config()
```


### Aufbereitung der Daten ###
```{r}
# Einlesen der Daten
nds_mod <- read_csv("datensatz_final_mod.csv")


nds_mod$Wochentag <- factor(nds_mod$Wochentag)
nds_mod$Season <- factor(nds_mod$Season)
nds_mod$Wetter <- factor(nds_mod$Wetter)
str(nds_mod)

#nds <- nds_mod[nds_mod$Umsatz>0,]
nds <- nds_mod

nds_names <- names(nds)

for(i in nds_names)
  {
    print(paste(sum(is.na(select(nds,i))),"NAs in ",i))
  }

nds_names

#nds_dummy_names <- nds_names[c(3,5,10,13)]
nds_dummy_names <- nds_names[c(3,5,10,13,14,17,18)]
nds_dummy_names
#nds_norm_names <- nds_names[c(4,6,7,8,9,11,12)]
nds_norm_names <- nds_names[c(4,6,7,8,9,11)]
nds_norm_names

```

```{r}
# Holzhammer ersetzt NAs durch Mittelwerte
#mTemp <- mean(nds$Temperatur, na.rm = TRUE)
#mBew <- round(mean(nds$Bewoelkung, na.rm = TRUE))
#mWet <- round(mean(nds$Wettercode, na.rm = TRUE)) # 33 Starker Sandsturm :-P
#mWind <- round(mean(nds$Windgeschwindigkeit, na.rm = TRUE))
#repBewoelkung <- mBew
#repTemperatur <- mTemp
#repWettercode <- 50 #Unterbrochener leichter und nicht gefrierender Sprühregen
#repWind <- mWind
#if(any(is.na(nds$Wettercode))) nds[is.na(nds$Wettercode),]$Wettercode <- repWettercode
#if(any(is.na(nds$Bewoelkung))) nds[is.na(nds$Bewoelkung),]$Bewoelkung <- repBewoelkung
#if(any(is.na(nds$Temperatur))) nds[is.na(nds$Temperatur),]$Temperatur <- repTemperatur
#if(any(is.na(nds$Windgeschwindigkeit)))
#  {
#    nds[is.na(nds$Windgeschwindigkeit),]$Windgeschwindigkeit <- repTemperatur
#  }
any(is.na(nds))

#write_csv(nds, "datensatz_final_mod.csv")

# Rekodierung von kategoriellen Variablen (zu Dummy-Variablen)
#dummy_list <- c("view", "waterfront")
nds_dummy <- dummy_cols(nds, nds_dummy_names)
names(nds_dummy)

# Standardisierung von metrischen Variablen
#norm_list <- c("price", "sqft_lot", "bathrooms", "grade", "condition")


# Berechnung der Mittelwerte und Standardabweichungen der zu standardisierenden Variablen
#norm_values_list <- get.norm_values(house_pricing_dummy, norm_list)
nds_norm_values <- get.norm_values(nds_dummy, nds_norm_names)
# Standardisierung der angegebenen metrischen Variablen
#house_pricing_norm <- norm_cols(house_pricing_dummy, norm_values_list)
nds_norm <- norm_cols(nds_dummy, nds_norm_values)

target_date <- as.Date("2019-06-01")
target_df <- nds_norm[nds_norm$Datum==target_date,]

nds_norm <- nds_norm[nds_norm$Umsatz>0,]

# Definition von Variablenlisten, um das Arbeiten mit diesen zu erleichtern
#waterfront_dummies = c('waterfront_0', 'waterfront_1')
#view_dummies = c('view_0', 'view_1', 'view_2', 'view_3','view_4')
Bewoelkung_dummies <- str_c("Bewoelkung_",unique(nds$Bewoelkung))
Warengruppe_dummies <- str_c("Warengruppe_",unique(nds$Warengruppe))
Wettercode_dummies <- str_c("Wettercode_",unique(nds$Wettercode))
Wochentag_dummies <- str_c("Wochentag_",unique(nds$Wochentag))
Wetter_dummies <- str_c("Wetter_",unique(nds$Wetter))
Monat_dummies <- str_c("Monat_",unique(nds$Monat))
Season_dummies <- str_c("Season_",unique(nds$Season))

# Definition der Label-Variable (der abhaengigen Variable, die vorhergesagt werden soll) sowie
label = "Umsatz_norm"
# Definition der Features (der unabhängigen Variablen auf deren Basis die Vorhersagen erzeugt werden sollen)

nds_normnames <- setdiff(names(nds_norm),names(nds_dummy))

#features <- c(setdiff(nds_normnames,label), Bewoelkung_dummies, Warengruppe_dummies, Wettercode_dummies, Wochentag_dummies, Wetter_dummies, Monat_dummies, Season_dummies)

features <- c(setdiff(nds_normnames,label), 
              Warengruppe_dummies, Wochentag_dummies, Monat_dummies)


#nds_normnames
#features
# Zufallszähler setzen, um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten
set.seed(1)
# Bestimmung der Indizes des Traininsdatensatzes
train_ind <- sample(seq_len(nrow(nds_norm)), size = floor(0.80 * nrow(nds_norm)))
#train_ind <- nds[nds$Datum!=target_date,]$X1 # alles bekannte als trainigsdaten?!
#train_ind
#nds_norm[nds_norm$Datum==target_date,]
# Teilen in Trainings- und Testdatensatz
train_dataset <- nds_norm[train_ind, features]
test_dataset <- nds_norm[-train_ind, features]
# Selektion der Variable, die als Label definiert wurde
train_labels <- nds_norm[train_ind, label]
test_labels <- nds_norm[-train_ind, label]

ncol(train_dataset)

#target_df <- nds_norm[nds_norm$Datum==target_date,]

features
predi <- function(x, mp )
  {
     ((mp(x) * nds_norm_values$sd[1]) + nds_norm_values$sd[1])
  }
#target_df$VU <- predi( target_df[c(features)],py$model$predict)

#View(nds_norm)
```


### Schätzung des Neuronalen Netzes
```{python}
# Benoetigte Python Libraries einbinden
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Definition der Form des tiefen neuronalen Netzes (Deep Neural Nets)
model = keras.Sequential([
  layers.Dense(120, activation='relu', input_shape=[len(r.train_dataset.keys())]),
  layers.Dense(30, activation='relu'),
  layers.Dense(1)
])

# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model.compile(loss="mse",
              optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))

# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model.summary()
```

```{python}
# Schaetzung des Modells
history = model.fit(r.train_dataset, r.train_labels, epochs=1500, validation_split = 0.1, verbose=0)

```


### Speichern des Neuronalen Netzes für spätere Vorhersagen ###
```{python}
#model.save("python_backen_model3500_x120x30x1_v001.h5")

```


### Auswertung der Modelloptimierung ###
```{r}
# Grafische Ausgabe der Modelloptimierung

# create data
data <- data.frame(val_loss = unlist(py$history$history$val_loss),
                  loss = unlist(py$history$history$loss))

# Plot
ggplot(data[-1,]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") 


```


### Laden eines gespeicherten Neuronalen Netzes ###
```{python}
model = keras.models.load_model("python_backen_model3500_x120x30x1_v001.h5")

```


### Auswertung der Schätzergebnisse ###
```{r}
# Schätzung der (normierten) Preise für die Trainings- und Testdaten
train_predictions_norm <- py$model$predict(train_dataset)
test_predictions_norm <- py$model$predict(test_dataset)

# Rückberechnung der normierten Preisschätzungen zu den tatsächlichen Preisschätzungen bzw. Preisen

train_predictions <- (train_predictions_norm * nds_norm_values$sd[1] ) + nds_norm_values$sd[1]
test_predictions <- (test_predictions_norm * nds_norm_values$sd[1]) + nds_norm_values$sd[1]
# Selektion der zugehörigen tatsächlichen Preise
train_actuals <- nds_norm$Umsatz[train_ind]
test_actuals <- nds_norm$Umsatz[-train_ind]
sum(abs(train_actuals-train_predictions))/length(train_actuals)
sum(abs(test_actuals-test_predictions))/length(test_actuals)

# Vergleich der Gütekriterien für die Traingings- und Testdaten
cat(paste0("MAPE on the Training Data:\t", format(mape(train_actuals, train_predictions), digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Test Data:\t\t", format(mape(test_actuals, test_predictions), digits=3, nsmall=2)))


```

```{r}

## Grafischer vergleich der vorhergesagten und der tatsächlichen Preise für die Trainings- und Testdaten

# Zusammenstellung der Daten für die Plots
data_train <- data.frame(prediction = train_predictions, actual = train_actuals)
data_test <- data.frame(prediction = test_predictions, actual = test_actuals)

# Plot der Ergebnisse der Trainingsdaten
ggplot(data_train[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("Case Number") +
  ylab("Umsatz in €") 

# Plot der Ergebnisse der Testdaten
ggplot(data_test[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("Case Number") +
  ylab("Umsatz in €") 

```

```{r}
# Vorhersage für einen einzelnen Fall
target_df$Umsatz <- predi(target_df[features], py$model$predict)
target_df
#write_csv(target_df, "python_backen_model1500_x24x18x1_v001.csv")

```
